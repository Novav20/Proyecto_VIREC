{"cells":[{"cell_type":"markdown","id":"d98a88ce","metadata":{"id":"d98a88ce"},"source":["# **📓 01: Preprocesamiento y Organización del Dataset**\n","\n","Este notebook es el corazón del pipeline de gestión de datos. Su propósito es tomar los datos crudos y etiquetados y organizarlos en una estructura de directorios lista para el entrenamiento.\n","\n","**Tareas Principales:**\n","1.  **Sincronización:** Lee la **Hoja de Etiquetas** de Google Sheets y la compara con el registro maestro (`dataset_map.csv`) para identificar archivos nuevos y cambios en las etiquetas.\n","2.  **Separación por Fuente:** Procesa los datos de las fuentes `propia` y `externa` de manera independiente.\n","3.  **División Estratificada:** Divide los nuevos archivos en conjuntos de `train`, `validation` y `test`, asegurando que la proporción de clases se mantenga.\n","4.  **Organización de Archivos:** Mueve físicamente los archivos de imagen desde las carpetas `fotos_crudas` a la estructura final del dataset.\n","5.  **Actualización de Registro:** Guarda el estado final en `dataset_map.csv` para mantener la trazabilidad."]},{"cell_type":"markdown","source":["### **Configuración del Entorno y Búsqueda de Rutas**\n","\n","Esta celda se encarga de toda la preparación necesaria para ejecutar el pipeline de preprocesamiento. Realiza las siguientes tareas en orden:\n","\n","1.  **Montar Google Drive:** Conecta el sistema de archivos de tu Google Drive al entorno de Colab.\n","2.  **Localizar el Proyecto:** Busca dinámicamente la carpeta raíz del proyecto (`Proyecto_VIREC`) para asegurar que el código funcione para cualquier colaborador.\n","3.  **Instalar Dependencias:** Lee el archivo `requirements.txt` y instala todas las librerías de Python necesarias.\n","4.  **Importar Librerías:** Importa todos los módulos necesarios para el notebook.\n","5.  **Cargar Configuración:** Importa las variables personalizadas (como el nombre de la Hoja de Etiquetas y la lista de clases) desde tu archivo `config.py`.\n","6.  **Autenticar:** Solicita los permisos necesarios para que el script pueda leer tu Hoja de Cálculo de Google.\n","7.  **Definir Rutas:** Construye todas las rutas a los directorios de datos que se utilizarán en las siguientes celdas."],"metadata":{"id":"RcNLueZpzA3r"},"id":"RcNLueZpzA3r"},{"cell_type":"code","execution_count":1,"id":"ee020b0f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"ee020b0f","executionInfo":{"status":"ok","timestamp":1758114708757,"user_tz":300,"elapsed":61169,"user":{"displayName":"juan david julio","userId":"09880350231438945878"}},"outputId":"49996fb8-0646-46d4-91bf-6662c3cf5aae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Buscando la carpeta ancla 'Proyecto_VIREC'...\n","✅ ¡Proyecto encontrado! La ruta base es: /content/drive/MyDrive/Colab Notebooks/TalentoTech/Proyecto_VIREC\n","\n","Instalando dependencias...\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h✅ Dependencias instaladas.\n","✅ Configuración local importada desde config.py.\n","✅ Autenticación con Google Sheets exitosa.\n","✅ Rutas de directorios configuradas.\n","\n","✅ Entorno completamente configurado.\n"]}],"source":["# ====================================================================================\n","# @title PASO 1: SETUP, CONEXIÓN Y CONFIGURACIÓN\n","# ====================================================================================\n","import os\n","import sys\n","\n","# --- 1. Montar Google Drive ---\n","if not os.path.exists('/content/drive/MyDrive'):\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Google Drive ya está montado.\")\n","\n","# --- 2. Búsqueda Dinámica de la Ruta Base del Proyecto ---\n","NOMBRE_CARPETA_ANCLA = 'Proyecto_VIREC'\n","RUTA_BASE_PROYECTO = None\n","print(f\"\\nBuscando la carpeta ancla '{NOMBRE_CARPETA_ANCLA}'...\")\n","for root, dirs, files in os.walk('/content/drive/MyDrive'):\n","    if NOMBRE_CARPETA_ANCLA in dirs:\n","        RUTA_BASE_PROYECTO = os.path.join(root, NOMBRE_CARPETA_ANCLA)\n","        break\n","\n","if not RUTA_BASE_PROYECTO:\n","    raise FileNotFoundError(f\"❌ ERROR CRÍTICO: No se pudo encontrar la carpeta '{NOMBRE_CARPETA_ANCLA}'.\")\n","else:\n","    print(f\"✅ ¡Proyecto encontrado! La ruta base es: {RUTA_BASE_PROYECTO}\")\n","\n","    # --- 3. Instalar Dependencias desde requirements.txt ---\n","    ruta_requirements = os.path.join(RUTA_BASE_PROYECTO, 'requirements.txt')\n","    if os.path.exists(ruta_requirements):\n","        print(\"\\nInstalando dependencias...\")\n","        !pip install -r \"{ruta_requirements}\" -q\n","        print(\"✅ Dependencias instaladas.\")\n","    else:\n","        raise FileNotFoundError(f\"❌ ERROR: No se encontró el archivo 'requirements.txt'.\")\n","\n","# --- 4. Importar Librerías ---\n","import pandas as pd\n","import shutil\n","from sklearn.model_selection import train_test_split\n","from google.colab import auth\n","import gspread\n","from google.auth import default\n","\n","# --- 5. Importar la Configuración del Usuario ---\n","sys.path.append(RUTA_BASE_PROYECTO)\n","try:\n","    from config import NOMBRE_HOJA_ETIQUETAS, CLASES_MODELO\n","    print(\"✅ Configuración local importada desde config.py.\")\n","except ImportError:\n","    raise ImportError(\"❌ ERROR: No se encontró 'config.py'. Por favor, crea una copia de 'config.py.template' y rellénalo.\")\n","\n","# --- 6. Autenticación con Google Sheets ---\n","try:\n","    auth.authenticate_user()\n","    creds, _ = default()\n","    gc = gspread.authorize(creds)\n","    print(\"✅ Autenticación con Google Sheets exitosa.\")\n","except Exception as e:\n","    print(f\"⚠️ Advertencia: No se pudo autenticar con Google Sheets. Error: {e}\")\n","\n","# --- 7. Construcción Final de Rutas ---\n","RUTA_DATASET = os.path.join(RUTA_BASE_PROYECTO, 'dataset')\n","RUTA_FOTOS_CRUDAS_PROPIAS = os.path.join(RUTA_DATASET, 'fotos_crudas_propias')\n","RUTA_FOTOS_CRUDAS_EXTERNAS = os.path.join(RUTA_DATASET, 'fotos_crudas_externas')\n","RUTA_DATASET_FINAL_PROPIO = os.path.join(RUTA_DATASET, 'dataset_final_propio')\n","RUTA_DATASET_FINAL_EXTERNO = os.path.join(RUTA_DATASET, 'dataset_final_externo')\n","RUTA_REGISTRO = os.path.join(RUTA_BASE_PROYECTO, 'dataset_map.csv')\n","print(\"✅ Rutas de directorios configuradas.\")\n","print(\"\\n✅ Entorno completamente configurado.\")"]},{"cell_type":"markdown","id":"f3fa3ffa","metadata":{"id":"f3fa3ffa"},"source":["### **Ejecución del Pipeline de Organización**\n","Esta celda contiene la lógica principal del notebook. Ejecútala para sincronizar la hoja de etiquetas con las carpetas de datos y organizar los archivos en la estructura `train/validation/test`."]},{"cell_type":"code","execution_count":2,"id":"2d76f1ea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"2d76f1ea","executionInfo":{"status":"ok","timestamp":1758114756983,"user_tz":300,"elapsed":9221,"user":{"displayName":"juan david julio","userId":"09880350231438945878"}},"outputId":"4d6eef52-0cf9-430b-90a1-16afae1099a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Estructuras de carpetas verificadas/creadas.\n","Se encontraron 502 registros 'propios' y 0 'externos' directamente desde Google Sheets.\n","No se encontró registro previo. Se creará uno nuevo.\n","\n","============================================================\n","            PROCESANDO FUENTE: PROPIO\n","============================================================\n","No se detectaron cambios en las etiquetas de archivos existentes.\n","\n","Se encontraron 502 archivos nuevos para procesar.\n","\n","============================================================\n","            PROCESANDO FUENTE: EXTERNO\n","============================================================\n","No se detectaron cambios en las etiquetas de archivos existentes.\n","No hay archivos nuevos para añadir en esta fuente.\n","\n","✅ Proceso completado. El registro 'dataset_map.csv' ha sido actualizado.\n","\n","Conteo total de archivos en los datasets finales:\n","\n","Dataset Propio:\n","  - Total en train: 401 archivos\n","  - Total en validation: 50 archivos\n","  - Total en test: 51 archivos\n","\n","Dataset Externo:\n","  - Total en train: 0 archivos\n","  - Total en validation: 0 archivos\n","  - Total en test: 0 archivos\n"]}],"source":["# ====================================================================================\n","# @title PASO 2: EJECUTAR PIPELINE DE PREPROCESAMIENTO\n","# ====================================================================================\n","\n","def procesar_fuente(df_deseado_fuente, df_actual_completo, ruta_fotos_crudas, ruta_dataset_final, nombre_fuente):\n","    \"\"\"\n","    Procesa un subconjunto de datos (propio o externo), maneja cambios de etiquetas,\n","    divide los nuevos archivos y los mueve a su destino final.\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"            PROCESANDO FUENTE: {nombre_fuente.upper()}\")\n","    print(\"=\"*60)\n","\n","    df_actual_fuente = df_actual_completo[df_actual_completo['fuente'] == nombre_fuente]\n","    archivos_procesados = set(df_actual_fuente['nombre_archivo'])\n","    df_nuevos = df_deseado_fuente[~df_deseado_fuente['nombre_archivo'].isin(archivos_procesados)]\n","\n","    # 1. Identificar y procesar etiquetas cambiadas\n","    df_merged = pd.merge(df_deseado_fuente, df_actual_fuente, on='nombre_archivo', suffixes=('_deseado', '_actual'), how='inner')\n","    df_cambiados = df_merged[df_merged['etiqueta_deseado'] != df_merged['etiqueta_actual']]\n","\n","    if not df_cambiados.empty:\n","        print(f\"Se detectaron {len(df_cambiados)} archivos con etiquetas cambiadas. Corrigiendo...\")\n","        for _, row in df_cambiados.iterrows():\n","            archivo, etiqueta_vieja, etiqueta_nueva, split = row['nombre_archivo'], row['etiqueta_actual'], row['etiqueta_deseado'], row['split']\n","            if pd.notna(split):\n","                ruta_vieja = os.path.join(ruta_dataset_final, split, etiqueta_vieja, archivo)\n","                ruta_nueva = os.path.join(ruta_dataset_final, split, etiqueta_nueva, archivo)\n","                if os.path.exists(ruta_vieja):\n","                    shutil.move(ruta_vieja, ruta_nueva)\n","                    df_actual_completo.loc[df_actual_completo['nombre_archivo'] == archivo, 'etiqueta'] = etiqueta_nueva\n","                    print(f\"  - '{archivo}' movido de '{etiqueta_vieja}' a '{etiqueta_nueva}'.\")\n","    else:\n","        print(\"No se detectaron cambios en las etiquetas de archivos existentes.\")\n","\n","    # 2. Procesar archivos nuevos\n","    if df_nuevos.empty:\n","        print(\"No hay archivos nuevos para añadir en esta fuente.\")\n","        return pd.DataFrame()\n","\n","    print(f\"\\nSe encontraron {len(df_nuevos)} archivos nuevos para procesar.\")\n","\n","    # Lógica de división robusta\n","    train_df, validation_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n","    if len(df_nuevos) < 5:\n","        train_df = df_nuevos.copy()\n","    else:\n","        if df_nuevos['etiqueta'].nunique() < 2:\n","            train_df, temp_df = train_test_split(df_nuevos, test_size=0.2, random_state=42)\n","        else:\n","            train_df, temp_df = train_test_split(df_nuevos, test_size=0.2, random_state=42, stratify=df_nuevos['etiqueta'])\n","        if temp_df['etiqueta'].nunique() < 2 or temp_df['etiqueta'].value_counts().min() < 2:\n","            validation_df = temp_df.copy()\n","        else:\n","            validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['etiqueta'])\n","\n","    if not train_df.empty: train_df.loc[:, 'split'] = 'train'\n","    if not validation_df.empty: validation_df.loc[:, 'split'] = 'validation'\n","    if not test_df.empty: test_df.loc[:, 'split'] = 'test'\n","\n","    # Mover archivos\n","    for df_split in [train_df, validation_df, test_df]:\n","        if not df_split.empty:\n","            for _, row in df_split.iterrows():\n","                origen = os.path.join(ruta_fotos_crudas, row['nombre_archivo'])\n","                destino = os.path.join(ruta_dataset_final, row['split'], row['etiqueta'], row['nombre_archivo'])\n","                if os.path.exists(origen):\n","                    shutil.move(origen, destino)\n","\n","    # Preparar dataframe de retorno\n","    nuevos_procesados_list = [df for df in [train_df, validation_df, test_df] if not df.empty]\n","    if not nuevos_procesados_list: return pd.DataFrame()\n","\n","    nuevos_procesados = pd.concat(nuevos_procesados_list).copy()\n","    nuevos_procesados['fuente'] = nombre_fuente\n","    return nuevos_procesados[['nombre_archivo', 'etiqueta', 'split', 'fuente']]\n","\n","# --- INICIO DE LA EJECUCIÓN ---\n","SPLITS = ['train', 'validation', 'test']\n","\n","# Crear estructura de carpetas\n","for base_path in [RUTA_DATASET_FINAL_PROPIO, RUTA_DATASET_FINAL_EXTERNO]:\n","    for split in SPLITS:\n","        for clase in CLASES_MODELO:\n","            os.makedirs(os.path.join(base_path, split, clase), exist_ok=True)\n","print(\"✅ Estructuras de carpetas verificadas/creadas.\")\n","\n","# Cargar datos\n","try:\n","    # --- ¡IMPORTANTE! Nombre de la Hoja de Etiquetas ---\n","    hoja_etiquetas = gc.open(NOMBRE_HOJA_ETIQUETAS).worksheet(\"Lista de etiquetas\")\n","\n","    valores_crudos = hoja_etiquetas.get_all_values()\n","    df_deseado = pd.DataFrame(valores_crudos[1:], columns=valores_crudos[0])\n","\n","    df_deseado.dropna(subset=['etiqueta', 'fuente'], inplace=True)\n","    df_deseado_propio = df_deseado[df_deseado['fuente'] == 'propio'].copy()\n","    df_deseado_externo = df_deseado[df_deseado['fuente'] == 'externo'].copy()\n","    print(f\"Se encontraron {len(df_deseado_propio)} registros 'propios' y {len(df_deseado_externo)} 'externos' directamente desde Google Sheets.\")\n","\n","    try:\n","        df_actual = pd.read_csv(RUTA_REGISTRO)\n","        print(f\"Se encontró un registro con {len(df_actual)} archivos procesados.\")\n","    except FileNotFoundError:\n","        df_actual = pd.DataFrame(columns=['nombre_archivo', 'etiqueta', 'split', 'fuente'])\n","        print(\"No se encontró registro previo. Se creará uno nuevo.\")\n","    if 'fuente' not in df_actual.columns: df_actual['fuente'] = None\n","\n","    # Ejecutar procesamiento\n","    nuevos_propios = procesar_fuente(df_deseado_propio, df_actual, RUTA_FOTOS_CRUDAS_PROPIAS, RUTA_DATASET_FINAL_PROPIO, 'propio')\n","    nuevos_externos = procesar_fuente(df_deseado_externo, df_actual, RUTA_FOTOS_CRUDAS_EXTERNAS, RUTA_DATASET_FINAL_EXTERNO, 'externo')\n","\n","    # Actualizar registro\n","    df_actual_actualizado = pd.concat([df_actual, nuevos_propios, nuevos_externos], ignore_index=True).drop_duplicates(subset=['nombre_archivo'], keep='last')\n","    df_actual_actualizado.to_csv(RUTA_REGISTRO, index=False)\n","    print(f\"\\n✅ Proceso completado. El registro '{os.path.basename(RUTA_REGISTRO)}' ha sido actualizado.\")\n","\n","    # Verificación final\n","    print(\"\\nConteo total de archivos en los datasets finales:\")\n","    for nombre, ruta in [(\"Propio\", RUTA_DATASET_FINAL_PROPIO), (\"Externo\", RUTA_DATASET_FINAL_EXTERNO)]:\n","        print(f\"\\nDataset {nombre}:\")\n","        for split in SPLITS:\n","            total_split = sum(len(files) for r, d, files in os.walk(os.path.join(ruta, split)))\n","            print(f\"  - Total en {split}: {total_split} archivos\")\n","\n","except (gspread.exceptions.SpreadsheetNotFound, KeyError) as e:\n","    print(f\"❌ ERROR: No se pudo completar el proceso. Revisa que la Hoja de Cálculo '{NOMBRE_HOJA_ETIQUETAS}' exista y tenga las columnas correctas ('etiqueta', 'fuente'). Error: {e}\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}