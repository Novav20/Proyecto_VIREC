{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714c5d58",
   "metadata": {},
   "source": [
    "# **üìì 02: Entrenamiento y Evaluaci√≥n de Modelos**\n",
    "\n",
    "Este notebook es el motor del pipeline de MLOps. Su funci√≥n es tomar los datasets organizados por el script `01_Preprocesamiento` y ejecutar ciclos de entrenamiento de forma automatizada, bas√°ndose en las tareas definidas en la \"Hoja de Experimentos\".\n",
    "\n",
    "**Flujo de Trabajo:**\n",
    "1.  **Conexi√≥n y Configuraci√≥n:** Se conecta a Google Drive y a la Hoja de Experimentos.\n",
    "2.  **Selecci√≥n de Tarea:** Busca y carga la pr√≥xima tarea de entrenamiento con estado `PENDIENTE`.\n",
    "3.  **Carga de Datos:** Carga las im√°genes de la fuente especificada (`propio`, `externo` o `mixto`).\n",
    "4.  **Construcci√≥n del Modelo:** Define la arquitectura del modelo usando Transfer Learning.\n",
    "5.  **Entrenamiento:** Compila y entrena el modelo, guardando la mejor versi√≥n (`ModelCheckpoint`).\n",
    "6.  **Evaluaci√≥n:** Genera un conjunto completo de m√©tricas y visualizaciones (curvas de aprendizaje, matriz de confusi√≥n, etc.).\n",
    "7.  **Registro de Resultados:** Actualiza la Hoja de Experimentos con el estado `COMPLETADO` y los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 1: SETUP DEL ENTORNO Y AUTENTICACI√ìN\n",
    "# ====================================================================================\n",
    "\n",
    "# --- Instalaci√≥n de Librer√≠as ---\n",
    "!pip install gspread google-auth-oauthlib pytz -q\n",
    "\n",
    "# --- Importaciones Principales ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gspread\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Importaciones para Modelado ---\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# --- Importaciones para Evaluaci√≥n y Visualizaci√≥n ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from google.colab import auth, drive\n",
    "from google.auth import default\n",
    "\n",
    "# --- Montar Drive y Autenticar para Google Sheets ---\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Google Drive ya est√° montado.\")\n",
    "\n",
    "try:\n",
    "    auth.authenticate_user()\n",
    "    creds, _ = default()\n",
    "    gc = gspread.authorize(creds)\n",
    "    print(\"‚úÖ Autenticaci√≥n con Google Sheets exitosa.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Advertencia: No se pudo autenticar con Google Sheets. El registro de resultados podr√≠a fallar. Error: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nTensorFlow Version:\", tf.__version__)\n",
    "print(\"‚úÖ Entorno listo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0396095",
   "metadata": {},
   "source": [
    "### **Configuraci√≥n del Pipeline**\n",
    "Esta celda se encarga de toda la configuraci√≥n necesaria para el pipeline de entrenamiento. Encuentra la ruta del proyecto, define todos los directorios importantes y se conecta a la \"Hoja de Experimentos\" para cargar la cola de tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 2: CONFIGURACI√ìN DE RUTAS Y CONEXI√ìN A LA HOJA DE TAREAS\n",
    "# ====================================================================================\n",
    "\n",
    "# --- 1. B√∫squeda Din√°mica de la Ruta Base del Proyecto ---\n",
    "NOMBRE_CARPETA_ANCLA = 'Proyecto_VIREC'\n",
    "RUTA_BASE_PROYECTO = None\n",
    "print(f\"Buscando la carpeta ancla '{NOMBRE_CARPETA_ANCLA}'...\")\n",
    "for root, dirs, files in os.walk('/content/drive/MyDrive'):\n",
    "    if NOMBRE_CARPETA_ANCLA in dirs:\n",
    "        RUTA_BASE_PROYECTO = os.path.join(root, NOMBRE_CARPETA_ANCLA)\n",
    "        break\n",
    "\n",
    "if not RUTA_BASE_PROYECTO:\n",
    "    raise FileNotFoundError(f\"‚ùå ERROR CR√çTICO: No se pudo encontrar la carpeta '{NOMBRE_CARPETA_ANCLA}'.\")\n",
    "else:\n",
    "    print(f\"‚úÖ ¬°Proyecto encontrado! La ruta base es: {RUTA_BASE_PROYECTO}\")\n",
    "\n",
    "# --- 2. Configuraci√≥n de Rutas Din√°micas ---\n",
    "RUTA_DATASET_PROPIO = os.path.join(RUTA_BASE_PROYECTO, 'dataset', 'dataset_final_propio')\n",
    "RUTA_DATASET_EXTERNO = os.path.join(RUTA_BASE_PROYECTO, 'dataset', 'dataset_final_externo')\n",
    "RUTA_MODELOS = os.path.join(RUTA_BASE_PROYECTO, 'modelos_entrenados')\n",
    "\n",
    "# --- 3. Conexi√≥n a la Hoja de Experimentos ---\n",
    "NOMBRE_HOJA_EXPERIMENTOS = \"VIREC - Hoja de Experimentos\"\n",
    "NOMBRE_PESTANA_EXPERIMENTOS = \"Experimentos\"\n",
    "\n",
    "try:\n",
    "    hoja_experimentos = gc.open(NOMBRE_HOJA_EXPERIMENTOS).worksheet(NOMBRE_PESTANA_EXPERIMENTOS)\n",
    "    print(f\"‚úÖ Conexi√≥n exitosa a la Hoja de Experimentos: '{NOMBRE_HOJA_EXPERIMENTOS}'\")\n",
    "except Exception as e:\n",
    "    hoja_experimentos = None\n",
    "    print(f\"‚ùå ERROR: No se pudo conectar a la Hoja de Experimentos. El script no podr√° actualizar los resultados. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945a9fd",
   "metadata": {},
   "source": [
    "### **Funciones de Utilidad del Pipeline**\n",
    "Esta celda contiene las funciones principales que definen el flujo de trabajo de un experimento. Se han separado para mejorar la legibilidad y la reutilizaci√≥n del c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664fafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 3: FUNCIONES DE UTILIDAD DEL PIPELINE\n",
    "# ====================================================================================\n",
    "\n",
    "def cargar_proxima_tarea(hoja_experimentos):\n",
    "    \"\"\"\n",
    "    Lee la hoja de experimentos, encuentra la primera tarea 'PENDIENTE'\n",
    "    y devuelve la informaci√≥n de la tarea y el n√∫mero de fila a actualizar.\n",
    "    \"\"\"\n",
    "    print(\"Buscando pr√≥xima tarea pendiente en la Hoja de Experimentos...\")\n",
    "    try:\n",
    "        valores_crudos = hoja_experimentos.get_all_values()\n",
    "        df_experimentos = pd.DataFrame(valores_crudos[1:], columns=valores_crudos[0])\n",
    "        \n",
    "        tareas_pendientes = df_experimentos[df_experimentos['status'].str.strip().str.upper() == 'PENDIENTE']\n",
    "        \n",
    "        if tareas_pendientes.empty:\n",
    "            print(\"‚úÖ No se encontraron experimentos pendientes.\")\n",
    "            return None, -1, None\n",
    "            \n",
    "        siguiente_tarea = tareas_pendientes.iloc[0]\n",
    "        fila_a_actualizar = siguiente_tarea.name + 2\n",
    "        \n",
    "        print(f\"-> Tarea encontrada: '{siguiente_tarea['experiment_id']}' en la fila {fila_a_actualizar}.\")\n",
    "        return siguiente_tarea, fila_a_actualizar, df_experimentos.columns.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR al leer la Hoja de Experimentos: {e}\")\n",
    "        return None, -1, None\n",
    "\n",
    "def cargar_datasets(dataset_source, img_size, batch_size, color_mode):\n",
    "    \"\"\"\n",
    "    Carga los datasets de entrenamiento, validaci√≥n y prueba seg√∫n la fuente especificada.\n",
    "    \"\"\"\n",
    "    print(f\"Cargando datos para la fuente: '{dataset_source}'...\")\n",
    "    \n",
    "    def cargar_split(ruta_fuente, split_name):\n",
    "        ruta_split = os.path.join(ruta_fuente, split_name)\n",
    "        if not os.path.exists(ruta_split) or not os.listdir(ruta_split): return None\n",
    "        return tf.keras.utils.image_dataset_from_directory(\n",
    "            ruta_split, labels='inferred', label_mode='binary',\n",
    "            image_size=img_size, batch_size=batch_size, color_mode=color_mode,\n",
    "            shuffle=(split_name == 'train')\n",
    "        )\n",
    "\n",
    "    if dataset_source == 'propio':\n",
    "        train_ds = cargar_split(RUTA_DATASET_PROPIO, 'train')\n",
    "        val_ds = cargar_split(RUTA_DATASET_PROPIO, 'validation')\n",
    "        test_ds = cargar_split(RUTA_DATASET_PROPIO, 'test')\n",
    "    elif dataset_source == 'externo':\n",
    "        train_ds = cargar_split(RUTA_DATASET_EXTERNO, 'train')\n",
    "        val_ds = cargar_split(RUTA_DATASET_EXTERNO, 'validation')\n",
    "        test_ds = cargar_split(RUTA_DATASET_EXTERNO, 'test')\n",
    "    elif dataset_source == 'mixto':\n",
    "        train_p = cargar_split(RUTA_DATASET_PROPIO, 'train')\n",
    "        val_p = cargar_split(RUTA_DATASET_PROPIO, 'validation')\n",
    "        test_p = cargar_split(RUTA_DATASET_PROPIO, 'test')\n",
    "        train_e = cargar_split(RUTA_DATASET_EXTERNO, 'train')\n",
    "        val_e = cargar_split(RUTA_DATASET_EXTERNO, 'validation')\n",
    "        test_e = cargar_split(RUTA_DATASET_EXTERNO, 'test')\n",
    "        \n",
    "        train_ds = train_p.concatenate(train_e) if train_p and train_e else (train_p or train_e)\n",
    "        val_ds = val_p.concatenate(val_e) if val_p and val_e else (val_p or val_e)\n",
    "        test_ds = test_p.concatenate(test_e) if test_p and test_e else (test_p or test_e)\n",
    "    else:\n",
    "        raise ValueError(f\"Fuente de datos '{dataset_source}' no reconocida.\")\n",
    "\n",
    "    if train_ds is None:\n",
    "        raise FileNotFoundError(f\"No se pudieron cargar datos de entrenamiento para la fuente '{dataset_source}'.\")\n",
    "\n",
    "    # Optimizaci√≥n del pipeline\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    if val_ds: val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    if test_ds: test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "def construir_modelo(img_size, channels, use_augmentation, modelo_base_nombre):\n",
    "    \"\"\"\n",
    "    Construye el modelo de Transfer Learning, seleccionando din√°micamente\n",
    "    la arquitectura base.\n",
    "    \"\"\"\n",
    "    print(f\"\\nConstruyendo la arquitectura del modelo con base: '{modelo_base_nombre}'...\")\n",
    "    \n",
    "    # Capas de preprocesamiento\n",
    "    rescale_layer = layers.Rescaling(1./255)\n",
    "    data_augmentation = Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ], name=\"data_augmentation\")\n",
    "\n",
    "    # --- L√ìGICA DE SELECCI√ìN DEL MODELO BASE ---\n",
    "    pesos_imagenet = 'imagenet' if channels == 3 else None\n",
    "    \n",
    "    if modelo_base_nombre.lower() == 'mobilenetv2':\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(img_size[0], img_size[1], channels),\n",
    "            include_top=False,\n",
    "            weights=pesos_imagenet\n",
    "        )\n",
    "    elif modelo_base_nombre.lower() == 'resnet50':\n",
    "        # Ejemplo de c√≥mo podr√≠as a√±adir otro modelo en el futuro\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            input_shape=(img_size[0], img_size[1], channels),\n",
    "            include_top=False,\n",
    "            weights=pesos_imagenet\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo base '{modelo_base_nombre}' no reconocido. Opciones v√°lidas: 'MobileNetV2', 'ResNet50', etc.\")\n",
    "    \n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Construir el pipeline de capas\n",
    "    inputs = keras.Input(shape=(img_size[0], img_size[1], channels))\n",
    "    x = inputs\n",
    "    if use_augmentation:\n",
    "        x = data_augmentation(x)\n",
    "    x = rescale_layer(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    print(\"‚úÖ Modelo construido exitosamente.\")\n",
    "    if pesos_imagenet:\n",
    "        print(f\"   -> Se utilizaron los pesos pre-entrenados de ImageNet para {modelo_base_nombre}.\")\n",
    "    else:\n",
    "        print(f\"   -> El modelo {modelo_base_nombre} se inicializ√≥ con pesos aleatorios.\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def entrenar_modelo(model, train_ds, val_ds, epochs, learning_rate, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Compila y entrena el modelo, guardando la mejor versi√≥n.\n",
    "    \"\"\"\n",
    "    print(\"\\nCompilando y preparando el entrenamiento...\")\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # --- Preparar los callbacks y argumentos ---\n",
    "    fit_kwargs = {'epochs': epochs}\n",
    "    \n",
    "    # Si hay datos de validaci√≥n, configuramos el checkpoint para monitorearlos\n",
    "    if val_ds:\n",
    "        monitor_metric = 'val_accuracy'\n",
    "        monitor_mode = 'max'\n",
    "        fit_kwargs['validation_data'] = val_ds\n",
    "    # Si no, monitoreamos la p√©rdida de entrenamiento\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: No se encontraron datos de validaci√≥n.\")\n",
    "        print(\"   El ModelCheckpoint guardar√° el modelo con la menor p√©rdida de entrenamiento ('loss').\")\n",
    "        monitor_metric = 'loss'\n",
    "        monitor_mode = 'min'\n",
    "        \n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=False,\n",
    "        monitor=monitor_metric,\n",
    "        mode=monitor_mode,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    fit_kwargs['callbacks'] = [model_checkpoint_callback]\n",
    "\n",
    "    # --- Iniciar el entrenamiento ---\n",
    "    print(f\"\\nIniciando entrenamiento por {epochs} √©pocas...\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        **fit_kwargs\n",
    "    )\n",
    "    print(\"\\n‚úÖ Entrenamiento completado.\")\n",
    "    return history\n",
    "\n",
    "def evaluar_y_registrar(history, test_ds, checkpoint_path, experiment_id, class_names):\n",
    "    \"\"\"\n",
    "    Eval√∫a el mejor modelo guardado, genera TODAS las visualizaciones (incluyendo an√°lisis de errores),\n",
    "    y devuelve un diccionario con las m√©tricas finales.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"INICIANDO FASE DE EVALUACI√ìN Y REGISTRO\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # 1. Cargar el mejor modelo guardado\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå ERROR: No se encontr√≥ el modelo guardado. Saltando evaluaci√≥n.\")\n",
    "        return {}\n",
    "    best_model = keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    # 2. Definir paleta de colores y estilo\n",
    "    PALETA_COLORES = {\"primario\": \"#3E8160\", \"secundario\": \"#A2D183\", \"acento\": \"#8AE2F8\", \"texto\": \"#312829\"}\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # 3. Visualizar y guardar curvas de aprendizaje\n",
    "    acc = history.history.get('accuracy', [])\n",
    "    val_acc = history.history.get('val_accuracy', [])\n",
    "    loss = history.history.get('loss', [])\n",
    "    val_loss = history.history.get('val_loss', [])\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    fig.suptitle(f'Resultados del Entrenamiento: {experiment_id}', fontsize=20, weight='bold', color=PALETA_COLORES['texto'])\n",
    "\n",
    "    ax1.plot(epochs_range, acc, 'o-', label='Entrenamiento', color=PALETA_COLORES['primario'], linewidth=2)\n",
    "    if val_acc:\n",
    "        ax1.plot(epochs_range, val_acc, 's--', label='Validaci√≥n', color=PALETA_COLORES['acento'], linewidth=2)\n",
    "        best_epoch = np.argmax(val_acc)\n",
    "        best_acc_val = val_acc[best_epoch]\n",
    "        ax1.axvline(x=best_epoch, color='gray', linestyle=':', linewidth=1.5, label=f'Mejor √âpoca ({best_epoch+1})')\n",
    "        ax1.annotate(f'{best_acc_val:.3f}', xy=(best_epoch, best_acc_val), xytext=(best_epoch, best_acc_val + 0.01), ha='center', color=PALETA_COLORES['primario'], weight='bold')\n",
    "    ax1.set_title('Precisi√≥n (Accuracy) por √âpoca', fontsize=16)\n",
    "    ax1.set_xlabel('√âpoca', fontsize=12)\n",
    "    ax1.set_ylabel('Precisi√≥n', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    ax2.plot(epochs_range, loss, 'o-', label='Entrenamiento', color=PALETA_COLORES['primario'], linewidth=2)\n",
    "    if val_loss:\n",
    "        ax2.plot(epochs_range, val_loss, 's--', label='Validaci√≥n', color=PALETA_COLORES['acento'], linewidth=2)\n",
    "        best_epoch_loss = np.argmin(val_loss)\n",
    "        best_loss_val = val_loss[best_epoch_loss]\n",
    "        ax2.axvline(x=best_epoch_loss, color='gray', linestyle=':', linewidth=1.5, label=f'Mejor √âpoca ({best_epoch_loss+1})')\n",
    "        ax2.annotate(f'{best_loss_val:.3f}', xy=(best_epoch_loss, best_loss_val), xytext=(best_epoch_loss, best_loss_val + 0.02), ha='center', color=PALETA_COLORES['primario'], weight='bold')\n",
    "    ax2.set_title('P√©rdida (Loss) por √âpoca', fontsize=16)\n",
    "    ax2.set_xlabel('√âpoca', fontsize=12)\n",
    "    ax2.set_ylabel('P√©rdida', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    ruta_grafica_aprendizaje = os.path.join(RUTA_RESULTADOS_EXPERIMENTO, 'curvas_de_aprendizaje.png')\n",
    "    plt.savefig(ruta_grafica_aprendizaje, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Gr√°fica de curvas de aprendizaje guardada en: {ruta_grafica_aprendizaje}\")\n",
    "    \n",
    "    # 4. Evaluar en el conjunto de prueba\n",
    "    resultados_finales = {}\n",
    "    if test_ds:\n",
    "        print(\"\\nEvaluando modelo con el conjunto de prueba...\")\n",
    "        test_loss, test_acc = best_model.evaluate(test_ds, verbose=0)\n",
    "        resultados_finales['test_accuracy'] = test_acc\n",
    "        resultados_finales['test_loss'] = test_loss\n",
    "        print(f'  -> Precisi√≥n (Accuracy) en Prueba: {test_acc:.2%}')\n",
    "\n",
    "        # --- INICIO DE LA L√ìGICA INTEGRADA DE AN√ÅLISIS DE ERRORES ---\n",
    "        print(\"\\nAnalizando errores del modelo...\")\n",
    "        all_images, all_labels, all_predictions, all_pred_probs = [], [], [], []\n",
    "        for images, labels in test_ds:\n",
    "            all_images.extend(images.numpy())\n",
    "            all_labels.extend(labels.numpy().flatten().astype(int))\n",
    "            preds = best_model.predict(images, verbose=0)\n",
    "            all_pred_probs.extend(preds.flatten())\n",
    "            all_predictions.extend((preds.flatten() > 0.5).astype(int))\n",
    "        \n",
    "        y_true = np.array(all_labels)\n",
    "        y_pred = np.array(all_predictions)\n",
    "\n",
    "        # Reporte de Clasificaci√≥n\n",
    "        print(\"\\n--- Reporte de Clasificaci√≥n ---\")\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "        report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
    "        \n",
    "        resultados_finales['test_precision'] = report_dict.get(class_names[1], {}).get('precision')\n",
    "        resultados_finales['test_recall'] = report_dict.get(class_names[1], {}).get('recall')\n",
    "        resultados_finales['test_f1_score'] = report_dict.get(class_names[1], {}).get('f1-score')\n",
    "\n",
    "        # Matriz de Confusi√≥n y Curva ROC\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        fig.suptitle(f'An√°lisis de Rendimiento en Prueba: {experiment_id}', fontsize=20, weight='bold', color=PALETA_COLORES['texto'])\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cmap_personalizado = sns.light_palette(PALETA_COLORES['secundario'], as_cmap=True)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap_personalizado, ax=ax1, xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16, \"weight\": \"bold\"}, cbar=False)\n",
    "        ax1.set_title('Matriz de Confusi√≥n', fontsize=16)\n",
    "        ax1.set_xlabel('Predicci√≥n', fontsize=12)\n",
    "        ax1.set_ylabel('Etiqueta Real', fontsize=12)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, all_pred_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax2.plot(fpr, tpr, color=PALETA_COLORES['primario'], lw=3, label=f'Curva ROC (AUC = {roc_auc:.3f})')\n",
    "        ax2.plot([0, 1], [0, 1], color=PALETA_COLORES['texto'], lw=1, linestyle=':')\n",
    "        ax2.fill_between(fpr, tpr, alpha=0.1, color=PALETA_COLORES['primario'])\n",
    "        ax2.set_xlim([-0.05, 1.05])\n",
    "        ax2.set_ylim([-0.05, 1.05])\n",
    "        ax2.set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12)\n",
    "        ax2.set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12)\n",
    "        ax2.set_title('Curva ROC', fontsize=16)\n",
    "        ax2.legend(loc=\"lower right\")\n",
    "        ax2.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        ruta_grafica_analisis = os.path.join(RUTA_RESULTADOS_EXPERIMENTO, 'analisis_en_prueba.png')\n",
    "        plt.savefig(ruta_grafica_analisis, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"‚úÖ Gr√°fica de an√°lisis en prueba guardada en: {ruta_grafica_analisis}\")\n",
    "        resultados_finales['test_auc'] = roc_auc\n",
    "\n",
    "        # Visualizaci√≥n de errores\n",
    "        error_indices = np.where(y_pred != y_true)[0]\n",
    "        \n",
    "        if len(error_indices) == 0:\n",
    "            print(\"‚úÖ No se encontraron errores de clasificaci√≥n en el conjunto de prueba.\")\n",
    "        else:\n",
    "            print(f\"Se encontraron {len(error_indices)} errores. Visualizando una muestra...\")\n",
    "            num_errors_to_show = min(len(error_indices), 16)\n",
    "            num_cols = 4\n",
    "            num_rows = (num_errors_to_show + num_cols - 1) // num_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 5))\n",
    "            fig.suptitle(f'Muestra de Predicciones Incorrectas - {experiment_id}', fontsize=20, weight='bold')\n",
    "            \n",
    "            for i, ax in enumerate(axes.flat):\n",
    "                if i < len(error_indices):\n",
    "                    idx = error_indices[i]\n",
    "                    true_class_name = class_names[all_labels[idx]]\n",
    "                    pred_class_name = class_names[all_predictions[idx]]\n",
    "                    \n",
    "                    ax.imshow(all_images[idx].astype(\"uint8\"), cmap='gray' if all_images[idx].shape[-1]==1 else None)\n",
    "                    ax.set_title(\n",
    "                        f\"Real: {true_class_name.capitalize()}\\nPred: {pred_class_name.capitalize()}\\n(Conf: {all_pred_probs[idx]:.1%})\",\n",
    "                        fontsize=12, color='white', backgroundcolor='#dc3545'\n",
    "                    )\n",
    "                    ax.axis(\"off\")\n",
    "                else:\n",
    "                    ax.axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            ruta_grafica_errores = os.path.join(RUTA_RESULTADOS_EXPERIMENTO, 'analisis_de_errores.png')\n",
    "            plt.savefig(ruta_grafica_errores, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"‚úÖ Gr√°fica de an√°lisis de errores guardada.\")\n",
    "        # --- FIN DE LA L√ìGICA INTEGRADA ---\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron datos de prueba. Saltando evaluaci√≥n final.\")\n",
    "\n",
    "    resultados_finales['best_val_accuracy'] = max(history.history.get('val_accuracy', [None]))\n",
    "    \n",
    "    return resultados_finales\n",
    "\n",
    "def actualizar_registros(hoja_experimentos, fila_a_actualizar, headers, resultado_final_experimento):\n",
    "    \"\"\"\n",
    "    Actualiza la Hoja de Experimentos y el CSV local con los resultados finales.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"INICIANDO FASE DE REGISTRO DE RESULTADOS\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # --- 1. Actualizar la Hoja de Experimentos en Google Sheets ---\n",
    "    try:\n",
    "        if hoja_experimentos:\n",
    "            batch_update_requests = []\n",
    "            # Marcar el estado como 'COMPLETADO'\n",
    "            status_col_index = headers.index('status') + 1\n",
    "            batch_update_requests.append({\n",
    "                'range': gspread.utils.rowcol_to_a1(fila_a_actualizar, status_col_index),\n",
    "                'values': [['COMPLETADO']]\n",
    "            })\n",
    "            \n",
    "            # Preparar la actualizaci√≥n para las dem√°s columnas\n",
    "            for col_name, value in resultado_final_experimento.items():\n",
    "                if col_name in headers and col_name not in ['experiment_id', 'status']:\n",
    "                    col_index = headers.index(col_name) + 1\n",
    "                    batch_update_requests.append({\n",
    "                        'range': gspread.utils.rowcol_to_a1(fila_a_actualizar, col_index),\n",
    "                        'values': [[str(value)]]\n",
    "                    })\n",
    "            \n",
    "            if batch_update_requests:\n",
    "                hoja_experimentos.batch_update(batch_update_requests, value_input_option='USER_ENTERED')\n",
    "            \n",
    "            print(f\"‚úÖ Fila {fila_a_actualizar} de la Hoja de Experimentos actualizada a 'COMPLETADO'.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No se pudo actualizar la Hoja de Experimentos (no conectada).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si la actualizaci√≥n de la hoja falla, lo marcamos como 'FALLIDO'\n",
    "        if hoja_experimentos:\n",
    "            status_col_index = headers.index('status') + 1\n",
    "            hoja_experimentos.update_cell(fila_a_actualizar, status_col_index, 'FALLIDO')\n",
    "            try:\n",
    "                log_col_index = headers.index('log_error') + 1\n",
    "                hoja_experimentos.update_cell(fila_a_actualizar, log_col_index, f\"Error al registrar: {e}\")\n",
    "            except ValueError: pass\n",
    "        print(f\"‚ùå Ocurri√≥ un error al actualizar la Hoja de Experimentos: {e}\")\n",
    "\n",
    "    # --- 2. Guardar/Actualizar el Registro Centralizado en un CSV (Anti-Duplicados) ---\n",
    "    registro_central_path = os.path.join(RUTA_MODELOS, 'registro_de_experimentos.csv')\n",
    "    df_resultado_nuevo = pd.DataFrame([resultado_final_experimento])\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(registro_central_path):\n",
    "            df_registro_existente = pd.read_csv(registro_central_path)\n",
    "            if resultado_final_experimento['experiment_id'] in df_registro_existente['experiment_id'].values:\n",
    "                df_registro_existente.set_index('experiment_id', inplace=True)\n",
    "                df_registro_existente.update(df_resultado_nuevo.set_index('experiment_id'))\n",
    "                df_registro_actualizado = df_registro_existente.reset_index()\n",
    "            else:\n",
    "                df_registro_actualizado = pd.concat([df_registro_existente, df_resultado_nuevo], ignore_index=True)\n",
    "        else:\n",
    "            df_registro_actualizado = df_resultado_nuevo\n",
    "\n",
    "        df_registro_actualizado.to_csv(registro_central_path, index=False)\n",
    "        print(f\"‚úÖ Registro de experimentos local actualizado en: {registro_central_path}\")\n",
    "        display(df_registro_actualizado.tail())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocurri√≥ un error al guardar el CSV de registro local: {e}\")\n",
    "        \n",
    "def imprimir_configuracion(experiment_id, fila, params):\n",
    "    \"\"\"\n",
    "    Imprime un resumen bien formateado de la configuraci√≥n del experimento\n",
    "    que se va a ejecutar.\n",
    "    \"\"\"\n",
    "    # Extraer los valores del diccionario de par√°metros (params)\n",
    "    dataset_source = params.get('DATASET_SOURCE', 'N/A')\n",
    "    base_model = params.get('MODELO_BASE', 'N/A')\n",
    "    learning_rate = params.get('LEARNING_RATE', 'N/A')\n",
    "    batch_size = params.get('BATCH_SIZE', 'N/A')\n",
    "    epochs = params.get('EPOCHS', 'N/A')\n",
    "    img_size = params.get('IMG_SIZE', 'N/A')\n",
    "    use_augmentation = params.get('USE_AUGMENTATION', 'N/A')\n",
    "    use_grayscale = params.get('USE_GRAYSCALE', 'N/A')\n",
    "    ruta_resultados = params.get('RUTA_RESULTADOS_EXPERIMENTO', 'N/A')\n",
    "    nombre_modelo = params.get('NOMBRE_ARCHIVO_MODELO', 'N/A')\n",
    "    \n",
    "    # Imprimir el resumen\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"          INICIANDO EXPERIMENTO: {experiment_id} (Fila {fila})\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"--- Configuraci√≥n Cargada ---\")\n",
    "    print(f\"  - Fuente de Datos:         '{dataset_source}'\")\n",
    "    print(f\"  - Modelo Base:         '{base_model}'\")\n",
    "    print(f\"  - Tasa de Aprendizaje (LR):  {learning_rate}\")\n",
    "    print(f\"  - Tama√±o de Lote (BS):       {batch_size}\")\n",
    "    print(f\"  - √âpocas de Entrenamiento:   {epochs}\")\n",
    "    print(f\"  - Tama√±o de Imagen:          {img_size}\")\n",
    "    print(f\"  - Aumento de Datos:          {use_augmentation}\")\n",
    "    print(f\"  - Escala de Grises:          {use_grayscale}\")\n",
    "    print(\"\\n--- Rutas de Salida ---\")\n",
    "    print(f\"  - Carpeta de Resultados:     {os.path.basename(ruta_resultados)}\")\n",
    "    print(f\"  - Archivo del Modelo:        {nombre_modelo}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Funciones de utilidad del pipeline definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64477fff",
   "metadata": {},
   "source": [
    "### **An√°lisis Exploratorio de Datos (EDA)**\n",
    "Esta secci√≥n es opcional y se puede ejecutar una sola vez para generar las visualizaciones del dataset. Las gr√°ficas se guardar√°n en una carpeta `informe` dentro de la ruta base del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title (OPCIONAL) PASO 3.5: EJECUTAR AN√ÅLISIS EXPLORATORIO DE DATOS\n",
    "# ====================================================================================\n",
    "\n",
    "def generar_visualizaciones_eda():\n",
    "    \"\"\"\n",
    "    Genera y guarda las gr√°ficas de EDA (muestra de im√°genes y balance de clases)\n",
    "    para el dataset propio.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando An√°lisis Exploratorio de Datos...\")\n",
    "    \n",
    "    # --- 1. Configuraci√≥n de Rutas para el Informe ---\n",
    "    ruta_eda = RUTA_DATASET_PROPIO # Hacemos el EDA sobre nuestro dataset principal\n",
    "    ruta_informe = os.path.join(RUTA_BASE_PROYECTO, 'informe')\n",
    "    os.makedirs(ruta_informe, exist_ok=True)\n",
    "    \n",
    "    # --- 2. Conteo de Clases  ---\n",
    "    class_counts = {}\n",
    "    total_images = 0\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_path = os.path.join(ruta_eda, split)\n",
    "        if os.path.exists(split_path):\n",
    "            for class_name in os.listdir(split_path):\n",
    "                class_path = os.path.join(split_path, class_name)\n",
    "                if os.path.isdir(class_path):\n",
    "                    num_files = len(os.listdir(class_path))\n",
    "                    class_counts[class_name] = class_counts.get(class_name, 0) + num_files\n",
    "                    total_images += num_files\n",
    "    \n",
    "    if total_images == 0:\n",
    "        print(\"No se encontraron im√°genes. Saltando EDA.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Generar y Guardar Gr√°ficas ---\n",
    "    \n",
    "    # L√≥gica para la muestra de im√°genes...\n",
    "    img_size = (224, 224) # Asumiendo un tama√±o de imagen por defecto\n",
    "    ruta_train_eda = os.path.join(ruta_eda, 'train')\n",
    "    if os.path.exists(ruta_train_eda) and os.listdir(ruta_train_eda):\n",
    "        # HACK: Usamos un generador temporal para no afectar los generadores principales\n",
    "        temp_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            ruta_train_eda,\n",
    "            labels='inferred',\n",
    "            label_mode='binary',\n",
    "            image_size=img_size,\n",
    "            batch_size=9, # 9 para la cuadr√≠cula de 3x3\n",
    "            shuffle=True\n",
    "        )\n",
    "        class_names = temp_ds.class_names\n",
    "        color_map = {class_names[0]: \"#dc3545\", class_names[1]: \"#3E8160\"} # Rojo para 'No reciclable', Verde para 'Reciclable'\n",
    "\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "        fig.suptitle(f'Muestra de Im√°genes - Fuente: Propio', fontsize=20, weight='bold')\n",
    "        \n",
    "        for images, labels in temp_ds.take(1): # Tomamos un solo batch\n",
    "            for i, ax in enumerate(axes.flat):\n",
    "                if i < len(images):\n",
    "                    # Extraer el nombre de la clase\n",
    "                    label_index = int(labels[i][0])\n",
    "                    class_name = class_names[label_index]\n",
    "                    \n",
    "                    # Configurar color del borde\n",
    "                    border_color = color_map.get(class_name, '#312829') # Color por defecto si hay m√°s clases\n",
    "\n",
    "                    # Mostrar la imagen\n",
    "                    ax.imshow(images[i].numpy().astype(\"uint8\"), cmap='gray' if images[i].shape[-1]==1 else None)\n",
    "                    ax.set_title(\n",
    "                        class_name.replace('_', ' ').capitalize(),\n",
    "                        fontsize=14,\n",
    "                        color=border_color,\n",
    "                        weight='bold'\n",
    "                    )\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "                    # A√±adir borde de color\n",
    "                    for spine in ax.spines.values():\n",
    "                        spine.set_edgecolor(border_color)\n",
    "                        spine.set_linewidth(4)\n",
    "                else:\n",
    "                    # Ocultar ejes si no hay imagen\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "        # Guardar la figura\n",
    "        ruta_grafica_muestra = os.path.join(ruta_informe, 'muestra_de_datos.png')\n",
    "        plt.savefig(ruta_grafica_muestra, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # L√≥gica para el balance de clases...\n",
    "    labels = sorted(class_counts.keys())\n",
    "    counts = [class_counts[label] for label in labels]\n",
    "    PALETA_COLORES = {\"texto\": \"#312829\", \"primario\": \"#3E8160\"}\n",
    "    colors = [PALETA_COLORES['texto'], PALETA_COLORES['primario']]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    bars = ax.barh(labels, counts, color=colors, height=0.6)\n",
    "\n",
    "    # T√≠tulos\n",
    "    fig.suptitle('Balance de Clases del Dataset', fontsize=20, weight='bold')\n",
    "    ax.set_title(f'Fuente: Propio | Total: {total_images} Im√°genes', fontsize=14)\n",
    "\n",
    "    # Invertir eje Y para que la clase m√°s com√∫n est√© arriba\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # A√±adir etiquetas con conteo y porcentaje\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        percentage = f'{(width / total_images) * 100:.1f}%'\n",
    "        ax.text(\n",
    "            width * 0.98,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'{width} ({percentage})',\n",
    "            ha='right',\n",
    "            va='center',\n",
    "            fontsize=14,\n",
    "            color='white',\n",
    "            weight='bold'\n",
    "        )\n",
    "\n",
    "    # Limpieza de estilo del gr√°fico\n",
    "    for spine_pos in ['top', 'right', 'bottom', 'left']:\n",
    "        ax.spines[spine_pos].set_visible(False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.tick_params(axis='y', length=0, labelsize=12)\n",
    "\n",
    "    # L√≠nea de balance perfecto (si hay dos clases)\n",
    "    if len(labels) == 2:\n",
    "        ax.axvline(x=total_images / 2, color='gray', linestyle='--', linewidth=1, alpha=0.7)\n",
    "        ax.text(total_images / 2, -0.7, 'Balance Perfecto (50%)', ha='center', color='gray', style='italic')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    # Guardar la figura\n",
    "    ruta_grafica_balance = os.path.join(ruta_informe, 'balance_de_clases.png')\n",
    "    plt.savefig(ruta_grafica_balance, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Gr√°ficas de EDA guardadas en la carpeta: {ruta_informe}\")\n",
    "\n",
    "# --- Descomenta la siguiente l√≠nea para ejecutar el EDA ---\n",
    "# generar_visualizaciones_eda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b55462",
   "metadata": {},
   "source": [
    "### **Ejecuci√≥n del Pipeline de Entrenamiento Automatizado**\n",
    "Esta celda es el motor principal del notebook. Al ejecutarla, orquestar√° todo el proceso:\n",
    "1.  Buscar√° la pr√≥xima tarea `PENDIENTE` en la Hoja de Experimentos.\n",
    "2.  Si encuentra una, cargar√° los datos y los par√°metros correspondientes.\n",
    "3.  Construir√°, compilar√° y entrenar√° el modelo.\n",
    "4.  Evaluar√° el rendimiento y generar√° todas las gr√°ficas.\n",
    "5.  Registrar√° los resultados finales en la Hoja de Experimentos y en el CSV local.\n",
    "\n",
    "Si no hay tareas pendientes, el script simplemente lo informar√° y se detendr√°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 4: EJECUTAR EL PR√ìXIMO EXPERIMENTO DE LA COLA\n",
    "# ====================================================================================\n",
    "\n",
    "# --- 1. Cargar la Tarea ---\n",
    "if 'hoja_experimentos' in locals() and hoja_experimentos:\n",
    "    siguiente_tarea, fila_a_actualizar, headers = cargar_proxima_tarea(hoja_experimentos)\n",
    "else:\n",
    "    siguiente_tarea = None\n",
    "\n",
    "if siguiente_tarea is not None:\n",
    "    try:\n",
    "        # --- 2. Cargar Par√°metros del Experimento ---\n",
    "        EXPERIMENT_ID = siguiente_tarea['experiment_id']\n",
    "        MODELO_BASE = siguiente_tarea['modelo_base']\n",
    "        LEARNING_RATE = float(str(siguiente_tarea['learning_rate']).replace(',', '.'))\n",
    "        BATCH_SIZE = int(siguiente_tarea['batch_size'])\n",
    "        EPOCHS = int(siguiente_tarea['epochs'])\n",
    "        img_size_int = int(siguiente_tarea['img_size'])\n",
    "        IMG_SIZE = (img_size_int, img_size_int)\n",
    "        USE_AUGMENTATION = str(siguiente_tarea['use_augmentation']).strip().upper() == 'TRUE'\n",
    "        USE_GRAYSCALE = str(siguiente_tarea['use_grayscale']).strip().upper() == 'TRUE'\n",
    "        DATASET_SOURCE = siguiente_tarea['dataset_source'].lower()\n",
    "        \n",
    "        # Crear carpetas y nombres de archivo para el experimento\n",
    "        timestamp_folder = datetime.now(pytz.timezone('America/Bogota')).strftime(\"%Y-%m-%d\")\n",
    "        NOMBRE_CARPETA_EXPERIMENTO = f\"{timestamp_folder}_{EXPERIMENT_ID}\"\n",
    "        RUTA_RESULTADOS_EXPERIMENTO = os.path.join(RUTA_MODELOS, NOMBRE_CARPETA_EXPERIMENTO)\n",
    "        os.makedirs(RUTA_RESULTADOS_EXPERIMENTO, exist_ok=True)\n",
    "        aug_str = \"aug-T\" if USE_AUGMENTATION else \"aug-F\"\n",
    "        gray_str = \"gray-T\" if USE_GRAYSCALE else \"gray-F\"\n",
    "        NOMBRE_ARCHIVO_MODELO = f\"{timestamp_folder}_lr-{LEARNING_RATE}_bs-{BATCH_SIZE}_is-{img_size_int}_{aug_str}_{gray_str}_best.keras\"\n",
    "        checkpoint_path = os.path.join(RUTA_RESULTADOS_EXPERIMENTO, NOMBRE_ARCHIVO_MODELO)\n",
    "        \n",
    "        imprimir_configuracion(EXPERIMENT_ID, fila_a_actualizar, locals()) # Imprime un resumen bonito\n",
    "\n",
    "        # --- 3. Cargar y Preprocesar Datos ---\n",
    "        color_mode = \"grayscale\" if USE_GRAYSCALE else \"rgb\"\n",
    "        channels = 1 if USE_GRAYSCALE else 3\n",
    "        train_ds, val_ds, test_ds = cargar_datasets(DATASET_SOURCE, IMG_SIZE, BATCH_SIZE, color_mode)\n",
    "        class_names = train_ds.class_names\n",
    "        \n",
    "        # --- 4. Construir el Modelo ---\n",
    "        model = construir_modelo(IMG_SIZE, channels, USE_AUGMENTATION, MODELO_BASE)\n",
    "        \n",
    "        # --- 5. Entrenar el Modelo ---\n",
    "        history = entrenar_modelo(model, train_ds, val_ds, EPOCHS, LEARNING_RATE, checkpoint_path)\n",
    "        \n",
    "        # --- 6. Evaluar y Generar Artefactos ---\n",
    "        resultados_metricas = evaluar_y_registrar(history, test_ds, checkpoint_path, EXPERIMENT_ID, class_names)\n",
    "        \n",
    "        # --- 7. Registrar Resultados Finales ---\n",
    "        # Combinamos los par√°metros del experimento con las m√©tricas obtenidas\n",
    "        resultado_final = {**siguiente_tarea, **resultados_metricas}\n",
    "        resultado_final['ruta_modelo_guardado'] = os.path.relpath(checkpoint_path, RUTA_BASE_PROYECTO)\n",
    "        resultado_final['fecha_completado'] = datetime.now(pytz.timezone('America/Bogota')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # Formatear n√∫meros para Sheets\n",
    "        for key, val in resultado_final.items():\n",
    "            if isinstance(val, float):\n",
    "                resultado_final[key] = str(val).replace('.', ',')\n",
    "\n",
    "        actualizar_registros(hoja_experimentos, fila_a_actualizar, headers, resultado_final)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si algo falla en cualquier punto, lo registramos en la hoja\n",
    "        print(f\"‚ùå FALLO EL EXPERIMENTO: {EXPERIMENT_ID}. Raz√≥n: {e}\")\n",
    "        if 'hoja_experimentos' in locals() and hoja_experimentos:\n",
    "            hoja_experimentos.update_cell(fila_a_actualizar, headers.index('status') + 1, 'FALLIDO')\n",
    "            try:\n",
    "                log_col_index = headers.index('log_error') + 1\n",
    "                hoja_experimentos.update_cell(fila_a_actualizar, log_col_index, str(e))\n",
    "            except ValueError: pass\n",
    "        # Re-lanzar el error para ver el traceback completo en Colab\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
