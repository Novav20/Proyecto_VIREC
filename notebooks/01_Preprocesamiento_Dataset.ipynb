{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98a88ce",
   "metadata": {},
   "source": [
    "# **üìì 01: Preprocesamiento y Organizaci√≥n del Dataset**\n",
    "\n",
    "Este notebook es el coraz√≥n del pipeline de gesti√≥n de datos. Su prop√≥sito es tomar los datos crudos y etiquetados y organizarlos en una estructura de directorios lista para el entrenamiento.\n",
    "\n",
    "**Tareas Principales:**\n",
    "1.  **Sincronizaci√≥n:** Lee el archivo `.csv` de etiquetas y lo compara con el registro maestro (`dataset_map.csv`) para identificar archivos nuevos y cambios en las etiquetas.\n",
    "2.  **Separaci√≥n por Fuente:** Procesa los datos de las fuentes `propia` y `externa` de manera independiente.\n",
    "3.  **Divisi√≥n Estratificada:** Divide los nuevos archivos en conjuntos de `train`, `validation` y `test`, asegurando que la proporci√≥n de clases se mantenga.\n",
    "4.  **Organizaci√≥n de Archivos:** Mueve f√≠sicamente los archivos de imagen desde las carpetas `fotos_crudas` a la estructura final del dataset.\n",
    "5.  **Actualizaci√≥n de Registro:** Guarda el estado final en `dataset_map.csv` para mantener la trazabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee020b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 1: SETUP DEL ENTORNO\n",
    "# ====================================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"‚úÖ Entorno listo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f6a36",
   "metadata": {},
   "source": [
    "### **Configuraci√≥n del Entorno y B√∫squeda de Rutas**\n",
    "Esta celda se encarga de dos tareas cr√≠ticas:\n",
    "1.  **Montar Google Drive.**\n",
    "2.  **Localizar din√°micamente la carpeta del proyecto** en Drive, asegurando que el notebook funcione para cualquier colaborador.\n",
    "3.  **Importar la configuraci√≥n** desde un archivo `config.py`. Este archivo contiene las rutas y IDs espec√≠ficos del entorno de cada usuario, separando la configuraci√≥n del c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 2: CONFIGURACI√ìN DE RUTAS Y CONEXI√ìN\n",
    "# ====================================================================================\n",
    "\n",
    "# Montar Google Drive\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Google Drive ya est√° montado.\")\n",
    "\n",
    "# --- B√∫squeda Din√°mica de la Ruta Base del Proyecto ---\n",
    "NOMBRE_CARPETA_ANCLA = 'Proyecto_VIREC'\n",
    "RUTA_BASE_PROYECTO = None\n",
    "print(f\"\\nBuscando la carpeta ancla '{NOMBRE_CARPETA_ANCLA}'...\")\n",
    "for root, dirs, files in os.walk('/content/drive/MyDrive'):\n",
    "    if NOMBRE_CARPETA_ANCLA in dirs:\n",
    "        RUTA_BASE_PROYECTO = os.path.join(root, NOMBRE_CARPETA_ANCLA)\n",
    "        break\n",
    "\n",
    "if not RUTA_BASE_PROYECTO:\n",
    "    raise FileNotFoundError(f\"‚ùå ERROR CR√çTICO: No se pudo encontrar la carpeta '{NOMBRE_CARPETA_ANCLA}'.\")\n",
    "else:\n",
    "    print(f\"‚úÖ ¬°Proyecto encontrado! La ruta base es: {RUTA_BASE_PROYECTO}\")\n",
    "    \n",
    "    # --- Importar la configuraci√≥n del usuario desde config.py ---\n",
    "    # Esto permite que cada usuario defina sus propias rutas sin modificar el c√≥digo.\n",
    "    import sys\n",
    "    sys.path.append(RUTA_BASE_PROYECTO)\n",
    "    try:\n",
    "        from config import ID_CARPETA_PROPIAS, ID_CARPETA_EXTERNAS\n",
    "        print(\"‚úÖ Configuraci√≥n local importada desde config.py.\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"‚ùå ERROR: No se encontr√≥ el archivo 'config.py'. Por favor, crea una copia de 'config.py.template', ren√≥mbrala y rellena tus IDs de carpeta.\")\n",
    "\n",
    "    # --- Construcci√≥n de todas las rutas del proyecto ---\n",
    "    RUTA_DATASET = os.path.join(RUTA_BASE_PROYECTO, 'dataset')\n",
    "    RUTA_FOTOS_CRUDAS_PROPIAS = os.path.join(RUTA_DATASET, 'fotos_crudas_propias')\n",
    "    RUTA_FOTOS_CRUDAS_EXTERNAS = os.path.join(RUTA_DATASET, 'fotos_crudas_externas')\n",
    "    RUTA_DATASET_FINAL_PROPIO = os.path.join(RUTA_DATASET, 'dataset_final_propio')\n",
    "    RUTA_DATASET_FINAL_EXTERNO = os.path.join(RUTA_DATASET, 'dataset_final_externo')\n",
    "    RUTA_CSV_ETIQUETAS = os.path.join(RUTA_BASE_PROYECTO, 'VIREC - Hoja de Etiquetas del Dataset.csv')\n",
    "    RUTA_REGISTRO = os.path.join(RUTA_BASE_PROYECTO, 'dataset_map.csv')\n",
    "    \n",
    "    print(\"‚úÖ Rutas configuradas exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa3ffa",
   "metadata": {},
   "source": [
    "### **Ejecuci√≥n del Pipeline de Organizaci√≥n**\n",
    "Esta celda contiene la l√≥gica principal del notebook. Ejec√∫tala para sincronizar el CSV de etiquetas con las carpetas de datos y organizar los archivos en la estructura `train/validation/test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# @title PASO 3: EJECUTAR PIPELINE DE PREPROCESAMIENTO\n",
    "# ====================================================================================\n",
    "\n",
    "def procesar_fuente(df_deseado_fuente, df_actual_completo, ruta_fotos_crudas, ruta_dataset_final, nombre_fuente):\n",
    "    \"\"\"\n",
    "    Procesa un subconjunto de datos (propio o externo), maneja cambios de etiquetas,\n",
    "    divide los nuevos archivos y los mueve a su destino final.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"            PROCESANDO FUENTE: {nombre_fuente.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_actual_fuente = df_actual_completo[df_actual_completo['fuente'] == nombre_fuente]\n",
    "    archivos_procesados = set(df_actual_fuente['nombre_archivo'])\n",
    "    df_nuevos = df_deseado_fuente[~df_deseado_fuente['nombre_archivo'].isin(archivos_procesados)]\n",
    "    \n",
    "    # 1. Identificar y procesar etiquetas cambiadas\n",
    "    df_merged = pd.merge(df_deseado_fuente, df_actual_fuente, on='nombre_archivo', suffixes=('_deseado', '_actual'), how='inner')\n",
    "    df_cambiados = df_merged[df_merged['etiqueta_deseado'] != df_merged['etiqueta_actual']]\n",
    "\n",
    "    if not df_cambiados.empty:\n",
    "        print(f\"Se detectaron {len(df_cambiados)} archivos con etiquetas cambiadas. Corrigiendo...\")\n",
    "        for _, row in df_cambiados.iterrows():\n",
    "            archivo, etiqueta_vieja, etiqueta_nueva, split = row['nombre_archivo'], row['etiqueta_actual'], row['etiqueta_deseado'], row['split']\n",
    "            if pd.notna(split):\n",
    "                ruta_vieja = os.path.join(ruta_dataset_final, split, etiqueta_vieja, archivo)\n",
    "                ruta_nueva = os.path.join(ruta_dataset_final, split, etiqueta_nueva, archivo)\n",
    "                if os.path.exists(ruta_vieja):\n",
    "                    shutil.move(ruta_vieja, ruta_nueva)\n",
    "                    df_actual_completo.loc[df_actual_completo['nombre_archivo'] == archivo, 'etiqueta'] = etiqueta_nueva\n",
    "                    print(f\"  - '{archivo}' movido de '{etiqueta_vieja}' a '{etiqueta_nueva}'.\")\n",
    "    else:\n",
    "        print(\"No se detectaron cambios en las etiquetas de archivos existentes.\")\n",
    "\n",
    "    # 2. Procesar archivos nuevos\n",
    "    if df_nuevos.empty:\n",
    "        print(\"No hay archivos nuevos para a√±adir en esta fuente.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nSe encontraron {len(df_nuevos)} archivos nuevos para procesar.\")\n",
    "    \n",
    "    # L√≥gica de divisi√≥n robusta\n",
    "    train_df, validation_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    if len(df_nuevos) < 5:\n",
    "        train_df = df_nuevos.copy()\n",
    "    else:\n",
    "        if df_nuevos['etiqueta'].nunique() < 2:\n",
    "            train_df, temp_df = train_test_split(df_nuevos, test_size=0.2, random_state=42)\n",
    "        else:\n",
    "            train_df, temp_df = train_test_split(df_nuevos, test_size=0.2, random_state=42, stratify=df_nuevos['etiqueta'])\n",
    "        if temp_df['etiqueta'].nunique() < 2 or temp_df['etiqueta'].value_counts().min() < 2:\n",
    "            validation_df = temp_df.copy()\n",
    "        else:\n",
    "            validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['etiqueta'])\n",
    "\n",
    "    if not train_df.empty: train_df.loc[:, 'split'] = 'train'\n",
    "    if not validation_df.empty: validation_df.loc[:, 'split'] = 'validation'\n",
    "    if not test_df.empty: test_df.loc[:, 'split'] = 'test'\n",
    "    \n",
    "    # Mover archivos\n",
    "    for df_split in [train_df, validation_df, test_df]:\n",
    "        if not df_split.empty:\n",
    "            for _, row in df_split.iterrows():\n",
    "                origen = os.path.join(ruta_fotos_crudas, row['nombre_archivo'])\n",
    "                destino = os.path.join(ruta_dataset_final, row['split'], row['etiqueta'], row['nombre_archivo'])\n",
    "                if os.path.exists(origen):\n",
    "                    shutil.move(origen, destino)\n",
    "    \n",
    "    # Preparar dataframe de retorno\n",
    "    nuevos_procesados_list = [df for df in [train_df, validation_df, test_df] if not df.empty]\n",
    "    if not nuevos_procesados_list: return pd.DataFrame()\n",
    "    \n",
    "    nuevos_procesados = pd.concat(nuevos_procesados_list).copy()\n",
    "    nuevos_procesados['fuente'] = nombre_fuente\n",
    "    return nuevos_procesados[['nombre_archivo', 'etiqueta', 'split', 'fuente']]\n",
    "\n",
    "# --- INICIO DE LA EJECUCI√ìN ---\n",
    "CLASES = ['reciclable', 'no_reciclable']\n",
    "SPLITS = ['train', 'validation', 'test']\n",
    "\n",
    "# Crear estructura de carpetas\n",
    "for base_path in [RUTA_DATASET_FINAL_PROPIO, RUTA_DATASET_FINAL_EXTERNO]:\n",
    "    for split in SPLITS:\n",
    "        for clase in CLASES:\n",
    "            os.makedirs(os.path.join(base_path, split, clase), exist_ok=True)\n",
    "print(\"‚úÖ Estructuras de carpetas verificadas/creadas.\")\n",
    "\n",
    "# Cargar datos\n",
    "try:\n",
    "    df_deseado = pd.read_csv(RUTA_CSV_ETIQUETAS)\n",
    "    df_deseado.dropna(subset=['etiqueta', 'fuente'], inplace=True)\n",
    "    df_deseado_propio = df_deseado[df_deseado['fuente'] == 'propio'].copy()\n",
    "    df_deseado_externo = df_deseado[df_deseado['fuente'] == 'externo'].copy()\n",
    "    print(f\"Se encontraron {len(df_deseado_propio)} registros 'propios' y {len(df_deseado_externo)} 'externos' en el CSV.\")\n",
    "    \n",
    "    try:\n",
    "        df_actual = pd.read_csv(RUTA_REGISTRO)\n",
    "        print(f\"Se encontr√≥ un registro con {len(df_actual)} archivos procesados.\")\n",
    "    except FileNotFoundError:\n",
    "        df_actual = pd.DataFrame(columns=['nombre_archivo', 'etiqueta', 'split', 'fuente'])\n",
    "        print(\"No se encontr√≥ registro previo. Se crear√° uno nuevo.\")\n",
    "    if 'fuente' not in df_actual.columns: df_actual['fuente'] = None\n",
    "    \n",
    "    # Ejecutar procesamiento\n",
    "    nuevos_propios = procesar_fuente(df_deseado_propio, df_actual, RUTA_FOTOS_CRUDAS_PROPIAS, RUTA_DATASET_FINAL_PROPIO, 'propio')\n",
    "    nuevos_externos = procesar_fuente(df_deseado_externo, df_actual, RUTA_FOTOS_CRUDAS_EXTERNAS, RUTA_DATASET_FINAL_EXTERNO, 'externo')\n",
    "    \n",
    "    # Actualizar registro\n",
    "    df_actual_actualizado = pd.concat([df_actual, nuevos_propios, nuevos_externos], ignore_index=True).drop_duplicates(subset=['nombre_archivo'], keep='last')\n",
    "    df_actual_actualizado.to_csv(RUTA_REGISTRO, index=False)\n",
    "    print(f\"\\n‚úÖ Proceso completado. El registro '{os.path.basename(RUTA_REGISTRO)}' ha sido actualizado.\")\n",
    "    \n",
    "    # Verificaci√≥n final\n",
    "    print(\"\\nConteo total de archivos en los datasets finales:\")\n",
    "    for nombre, ruta in [(\"Propio\", RUTA_DATASET_FINAL_PROPIO), (\"Externo\", RUTA_DATASET_FINAL_EXTERNO)]:\n",
    "        print(f\"\\nDataset {nombre}:\")\n",
    "        for split in SPLITS:\n",
    "            total_split = sum(len(files) for r, d, files in os.walk(os.path.join(ruta, split)))\n",
    "            print(f\"  - Total en {split}: {total_split} archivos\")\n",
    "    \n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"‚ùå ERROR: No se pudo completar el proceso. Revisa que el archivo CSV exista y tenga las columnas correctas ('etiqueta', 'fuente'). Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
